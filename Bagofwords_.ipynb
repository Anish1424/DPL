{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqbr8dNnKbpRJmRb5B33JO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ðŸ“˜ Continuous Bag of Words (CBOW) Implementation in NumPy\n","# Works perfectly in Google Colab\n","# Stages:\n","# a) Data Preparation\n","# b) Generate Training Data\n","# c) Train Model\n","# d) Output Word Embeddings and Predictions\n","\n","import numpy as np\n","from collections import defaultdict\n","\n","# -------------------- a) Data Preparation --------------------\n","corpus = \"\"\"we are what we repeatedly do excellence then is not an act but a habit\"\"\"\n","corpus = corpus.lower().split()\n","\n","# Build vocabulary\n","vocab = sorted(set(corpus))\n","vocab_size = len(vocab)\n","word2idx = {w: i for i, w in enumerate(vocab)}\n","idx2word = {i: w for w, i in word2idx.items()}\n","\n","print(\"Vocabulary:\", vocab)\n","print(\"Vocab size:\", vocab_size)\n","\n","# -------------------- b) Generate Training Data --------------------\n","window_size = 2  # number of context words on each side\n","data = []\n","\n","for i in range(len(corpus)):\n","    target = corpus[i]\n","    context = []\n","    for j in range(i - window_size, i + window_size + 1):\n","        if j != i and 0 <= j < len(corpus):\n","            context.append(corpus[j])\n","    data.append((context, target))\n","\n","print(\"\\nSample training pairs (context -> target):\")\n","for context, target in data[:5]:\n","    print(context, \"->\", target)\n","\n","# One-hot encoding\n","def one_hot(index, size=vocab_size):\n","    vec = np.zeros(size)\n","    vec[index] = 1\n","    return vec\n","\n","# Prepare training data\n","X = []  # context inputs (average one-hots)\n","Y = []  # target outputs (one-hot)\n","for context_words, target_word in data:\n","    x = np.mean([one_hot(word2idx[w]) for w in context_words], axis=0)\n","    y = one_hot(word2idx[target_word])\n","    X.append(x)\n","    Y.append(y)\n","\n","X = np.array(X)\n","Y = np.array(Y)\n","\n","print(\"\\nShapes:\")\n","print(\"X:\", X.shape)\n","print(\"Y:\", Y.shape)\n","\n","# -------------------- c) Train Model --------------------\n","np.random.seed(42)\n","D = 10  # embedding dimension\n","W1 = np.random.randn(vocab_size, D) * 0.01  # input -> hidden\n","W2 = np.random.randn(D, vocab_size) * 0.01  # hidden -> output\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return e_x / np.sum(e_x, axis=1, keepdims=True)\n","\n","def cross_entropy(pred, true):\n","    return -np.mean(np.sum(true * np.log(pred + 1e-10), axis=1))\n","\n","learning_rate = 0.1\n","epochs = 1000\n","\n","for epoch in range(1, epochs + 1):\n","    # Forward pass\n","    h = X.dot(W1)\n","    u = h.dot(W2)\n","    y_pred = softmax(u)\n","\n","    loss = cross_entropy(y_pred, Y)\n","\n","    # Backpropagation\n","    dL_du = (y_pred - Y) / X.shape[0]\n","    dW2 = h.T.dot(dL_du)\n","    dW1 = X.T.dot(dL_du.dot(W2.T))\n","\n","    # Update weights\n","    W1 -= learning_rate * dW1\n","    W2 -= learning_rate * dW2\n","\n","    if epoch % 200 == 0 or epoch == 1:\n","        print(f\"Epoch {epoch:4d} | Loss: {loss:.4f}\")\n","\n","print(\"\\nTraining complete!\")\n","\n","# -------------------- d) Output --------------------\n","# Word embeddings\n","embeddings = W1\n","\n","def cosine_similarity(vecs, v):\n","    return np.dot(vecs, v) / (np.linalg.norm(vecs, axis=1) * np.linalg.norm(v) + 1e-10)\n","\n","def nearest(word, top_n=3):\n","    idx = word2idx[word]\n","    v = embeddings[idx]\n","    sims = cosine_similarity(embeddings, v)\n","    nearest_ids = np.argsort(-sims)[1:top_n+1]\n","    return [idx2word[i] for i in nearest_ids]\n","\n","print(\"\\nNearest words by similarity:\")\n","for w in [\"we\", \"do\", \"habit\", \"excellence\"]:\n","    print(f\"{w:12s} -> {nearest(w)}\")\n","\n","# Predict the center word given context\n","def predict_center(context_words, top=3):\n","    context_vec = np.mean([one_hot(word2idx[w]) for w in context_words], axis=0).reshape(1, -1)\n","    h = context_vec.dot(W1)\n","    u = h.dot(W2)\n","    probs = softmax(u)[0]\n","    top_ids = np.argsort(-probs)[:top]\n","    return [(idx2word[i], probs[i]) for i in top_ids]\n","\n","print(\"\\nPrediction examples:\")\n","print(\"Context ['we', 'repeatedly'] ->\", predict_center(['we', 'repeatedly']))\n","print(\"Context ['not', 'an'] ->\", predict_center(['not', 'an']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AB03Eji23YnI","executionInfo":{"status":"ok","timestamp":1762847156756,"user_tz":-330,"elapsed":178,"user":{"displayName":"Suyog Raosaheb Daule","userId":"10411070278688886375"}},"outputId":"3ec41e81-de55-41df-ceeb-68b2c332f76c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['a', 'act', 'an', 'are', 'but', 'do', 'excellence', 'habit', 'is', 'not', 'repeatedly', 'then', 'we', 'what']\n","Vocab size: 14\n","\n","Sample training pairs (context -> target):\n","['are', 'what'] -> we\n","['we', 'what', 'we'] -> are\n","['we', 'are', 'we', 'repeatedly'] -> what\n","['are', 'what', 'repeatedly', 'do'] -> we\n","['what', 'we', 'do', 'excellence'] -> repeatedly\n","\n","Shapes:\n","X: (15, 14)\n","Y: (15, 14)\n","Epoch    1 | Loss: 2.6390\n","Epoch  200 | Loss: 2.6372\n","Epoch  400 | Loss: 2.6095\n","Epoch  600 | Loss: 2.2950\n","Epoch  800 | Loss: 1.8675\n","Epoch 1000 | Loss: 1.5046\n","\n","Training complete!\n","\n","Nearest words by similarity:\n","we           -> ['excellence', 'what', 'repeatedly']\n","do           -> ['repeatedly', 'are', 'what']\n","habit        -> ['act', 'but', 'a']\n","excellence   -> ['then', 'we', 'not']\n","\n","Prediction examples:\n","Context ['we', 'repeatedly'] -> [('are', np.float64(0.31918567256568237)), ('what', np.float64(0.24604673663066978)), ('repeatedly', np.float64(0.15802689600834488))]\n","Context ['not', 'an'] -> [('act', np.float64(0.14152860922019797)), ('habit', np.float64(0.13529057600464792)), ('an', np.float64(0.13123932752845013))]\n"]}]}]}